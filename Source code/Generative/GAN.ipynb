channels = 3
img_size = 64
latent_dim = 100
batch_size = 32

from torch import nn

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)


class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        self.init_size = img_size // 4
        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))

        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(128),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.BatchNorm2d(128, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, stride=1, padding=1),
            nn.BatchNorm2d(64, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, channels, 3, stride=1, padding=1),
            nn.Tanh(),
        )

    def forward(self, z):
        out = self.l1(z)
        out = out.view(out.shape[0], 128, self.init_size, self.init_size)
        img = self.conv_blocks(out)
        return img


class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        def discriminator_block(in_filters, out_filters, bn=True):
            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]
            if bn:
                block.append(nn.BatchNorm2d(out_filters, 0.8))
            return block

        self.model = nn.Sequential(
            *discriminator_block(channels, 16, bn=False),
            *discriminator_block(16, 32),
            *discriminator_block(32, 64),
            *discriminator_block(64, 128),
        )

        # The height and width of downsampled image
        ds_size = img_size // 2 ** 4
        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())

    def forward(self, img):
        out = self.model(img)
        out = out.view(out.shape[0], -1)
        validity = self.adv_layer(out)

        return validity


def move_model_to_GPU(model):
  model = nn.DataParallel(model)
  return model.to('cuda')

from tqdm import tqdm
import numpy as np
import torch
from torch.autograd import Variable

cuda = torch.cuda.is_available()

Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor

class GAN():
    def __init__(self, cuda=True):
        self.adversarial_loss = torch.nn.BCELoss()
        self.generator = move_model_to_GPU(Generator())
        self.discriminator = move_model_to_GPU(Discriminator())

        self.optimizer_G = torch.optim.Adam(self.generator.parameters())
        self.optimizer_D = torch.optim.Adam(self.discriminator.parameters())

    def generate(self, num_gen=1):
      # Sample noise as generator input
      z = Variable(Tensor(np.random.normal(0, 1, (num_gen, latent_dim))))
      # Generate a batch of images
      gen_imgs = self.generator(z)

      return gen_imgs

    def fit(self, dataloader, callbacks=[], n_epochs=100):
        for epoch in range(n_epochs):
            print(f'Epoch {epoch}')
            bar = tqdm(dataloader)
            i = 0
            running_loss = 0.0

            data_size = len(dataloader)

            for imgs, _ in bar:
                current_batch_size = imgs.size()[0]
                # Adversarial ground truths
                valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)
                fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)

                # Configure input
                real_imgs = Variable(imgs.type(Tensor))

                # -----------------
                #  Train Generator
                # -----------------

                self.optimizer_G.zero_grad()

                # Sample noise as generator input
                z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))

                # Generate a batch of images
                gen_imgs = self.generator(z)

                # Loss measures generator's ability to fool the discriminator
                g_loss = self.adversarial_loss(self.discriminator(gen_imgs), valid)

                g_loss.backward()
                self.optimizer_G.step()

                # ---------------------
                #  Train Discriminator
                # ---------------------

                self.optimizer_D.zero_grad()

                # Measure discriminator's ability to classify real from generated samples
                real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)
                fake_loss = self.adversarial_loss(self.discriminator(gen_imgs.detach()), fake)
                d_loss = (real_loss + fake_loss) / 2

                running_loss += d_loss.item()
                if i % 16 == 15:
                    last_loss = running_loss / current_batch_size # loss per batch
                    bar.set_description_str(f'batch {i + 1}/{data_size // current_batch_size} loss: {last_loss:.4f}')

                    running_loss = 0.

                i += 1

                d_loss.backward()
                self.optimizer_D.step()

            for callback in callbacks:
              callback(self)

from torchvision.datasets import LFWPeople
import torchvision.transforms as transforms
import torch

pil_transformer = transforms.Compose([
    transforms.PILToTensor()
])

train_ds = LFWPeople('/content/drive/MyDrive', split='train', download=True,
                        image_set='original', transform=lambda pil: pil_transformer(pil.resize(
                            (img_size, img_size))) / 255)

test_ds = LFWPeople('/content/drive/MyDrive', split='test', download=True,
                       image_set='original', transform=lambda pil: pil_transformer(pil.resize(
                            (img_size, img_size))) / 255)

ds = torch.utils.data.ConcatDataset([train_ds, test_ds])
# ds = torch.utils.data.Subset(train_ds, list(range(30)))

print(len(ds))

from matplotlib import pyplot as plt
import cv2
from sklearn.preprocessing import MinMaxScaler

plot_counts = 16
num_rows = 2
num_cols = plot_counts // num_rows

save_path = '/content/drive/MyDrive/GAN/gan_cnn.torch'

def plot(net):
    images = net.generate(num_gen=plot_counts)

    _, ax = plt.subplots(num_rows, num_cols, figsize=(10, 10))

    print(f'Plotting...')
    for i, image in enumerate(images):
        cpu_image = image.detach().cpu()
        # C, H, W
        cpu_image = torch.swapaxes(cpu_image, 0, 2)
        cpu_image = torch.swapaxes(cpu_image, 0, 1).numpy()
        ori_shape = cpu_image.shape

        cpu_image = MinMaxScaler().fit_transform(cpu_image.reshape((
            cpu_image.shape[0] * cpu_image.shape[1], cpu_image.shape[2]
        ))).reshape(ori_shape)

        if num_rows == 1:
            ax[i].imshow(cpu_image)
            ax[i].axis("off")
        else:
            ax[i // num_cols, i % num_cols].imshow(cpu_image)
            ax[i // num_cols, i % num_cols].axis("off")

    plt.tight_layout()
    plt.show()

def save_model(net):
  with open(save_path, 'wb') as f:
    torch.save(net.generator.module.state_dict(), f)

trainer = GAN()
loader = torch.utils.data.DataLoader(
             ds,
             batch_size=batch_size,
             shuffle=True,
             num_workers=12,
             persistent_workers=True)

trainer.fit(loader, callbacks=[plot, save_model])
